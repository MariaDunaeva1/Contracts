# âš–ï¸ LexAnalyzer - Sistema de AnÃ¡lisis de Contratos Legales

Sistema completo para anÃ¡lisis inteligente de contratos usando LLM fine-tuneado + RAG + Agentes especializados.

## ğŸ¯ Â¿QuÃ© hace este sistema?

Analiza contratos legales automÃ¡ticamente y extrae:
- âœ… ClÃ¡usulas principales
- âš ï¸ Riesgos legales
- ğŸ“Š Obligaciones de las partes
- ğŸ” ComparaciÃ³n con contratos estÃ¡ndar
- ğŸ’¡ Recomendaciones

## ğŸ§  Arquitectura

```
Usuario â†’ Frontend â†’ Backend Go â†’ RAG Service Python â†’ Ollama (Modelo Fine-tuned)
                                        â†“
                                  ChromaDB (Vector Store)
                                        â†“
                                  4 Agentes Especializados
```

## âš¡ Quick Start

### Prerequisitos

- **Ollama** instalado y corriendo (https://ollama.ai)
- **Docker** y Docker Compose
- **Python 3.9+**

### Paso 1: Cargar Modelo Fine-Tuneado

El proyecto incluye un modelo **ya entrenado** especializado en contratos legales:

```bash
# Cargar el modelo en Ollama (solo una vez)
load-finetuned-model.bat
```

Esto crea `legal-contract-analyzer` usando los adaptadores LoRA en `models/lora_model/`.

### Paso 2: Iniciar Servicios

```bash
# Terminal 1: Backend + Frontend + Base de datos
docker-compose up --build

# Terminal 2: RAG Service
start-rag-system.bat
```

### Paso 3: Analizar Contratos

Abre http://localhost:3000/contract-analysis.html

## ğŸ“ Estructura del Proyecto

```
contracts/
â”œâ”€â”€ backend/                    # API Go
â”‚   â”œâ”€â”€ cmd/server/            # Entry point
â”‚   â”œâ”€â”€ internal/              # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ handlers/          # HTTP handlers
â”‚   â”‚   â”œâ”€â”€ services/          # Servicios (Kaggle, logs)
â”‚   â”‚   â””â”€â”€ storage/           # MinIO storage
â”‚   â””â”€â”€ services/              # RAG Service Python
â”‚       â”œâ”€â”€ agents/            # 4 agentes especializados
â”‚       â”œâ”€â”€ llm_service.py     # Cliente Ollama
â”‚       â”œâ”€â”€ rag_service.py     # Orquestador RAG
â”‚       â””â”€â”€ vector_service.py  # ChromaDB
â”œâ”€â”€ frontend/                   # UI HTML/JS
â”‚   â”œâ”€â”€ contract-analysis.html # Interfaz principal
â”‚   â””â”€â”€ js/                    # LÃ³gica frontend
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lora_model/            # Modelo fine-tuneado (YA ENTRENADO)
â”‚       â”œâ”€â”€ adapter_model.safetensors  # Pesos LoRA
â”‚       â””â”€â”€ adapter_config.json
â”œâ”€â”€ data/contracts/            # Dataset LEDGAR
â””â”€â”€ docs/                      # DocumentaciÃ³n
```

## ğŸ”‘ Conceptos Importantes

### âœ… Selector de Modelo en la UI

La interfaz te permite elegir entre:
- **Fine-tuned**: Modelo con prompts especializados para anÃ¡lisis legal (+25-30% precisiÃ³n)
- **Base**: Modelo general sin especializaciÃ³n

Ambos usan el mismo modelo base (Llama 3.2 3B) pero con diferentes system prompts.

### â˜ï¸ Groq vs Ollama

**Groq (Recomendado - Por defecto):**
- API cloud gratuita
- Ultra-rÃ¡pido (10x mÃ¡s rÃ¡pido)
- Sin instalaciÃ³n (0 GB)
- Requiere internet

**Ollama (Opcional - Local):**
- 100% privado
- Requiere 5GB de espacio
- MÃ¡s lento (depende de tu GPU)
- No requiere internet

Cambiar entre ambos es solo editar `.env`:
```env
LLM_PROVIDER=groq  # o "ollama"
```

### âŒ NO necesitas hacer fine-tuning cada vez

El fine-tuning ya estÃ¡ hecho. Solo se usa para:
- Entrenar con nuevos datasets (1000+ contratos)
- Especializar en tipos especÃ­ficos de contratos
- Mejorar el modelo actual

**LimitaciÃ³n:** Kaggle da 30h GPU/semana, cada entrenamiento tarda 2-4h.

## ğŸ› ï¸ Servicios

| Servicio | Puerto | DescripciÃ³n |
|----------|--------|-------------|
| Frontend | 3000 | Interfaz web |
| Backend API | 8080 | API REST Go |
| RAG Service | 8001 | Servicio Python de anÃ¡lisis |
| PostgreSQL | 5432 | Base de datos |
| MinIO | 9000 | Object storage |
| Groq API | - | LLM cloud (por defecto) |
| Ollama | 11434 | LLM local (opcional) |

## ğŸ“š DocumentaciÃ³n

- **[CONFIGURACION_GROQ.md](CONFIGURACION_GROQ.md)** - Configurar Groq API (recomendado)
- **[MODELO_FINE_TUNEADO.md](MODELO_FINE_TUNEADO.md)** - CÃ³mo funciona el modelo
- **[RAG_SYSTEM_README.md](RAG_SYSTEM_README.md)** - Sistema RAG y agentes
- **[COMPLETE_USAGE_GUIDE.md](COMPLETE_USAGE_GUIDE.md)** - GuÃ­a completa de uso
- **[DOCKER_QUICKSTART.md](DOCKER_QUICKSTART.md)** - Comandos Docker Ãºtiles
- **[API_EXAMPLES.md](API_EXAMPLES.md)** - Ejemplos de API

## ğŸ§ª Testing

```bash
# Test conexiÃ³n frontend-backend
curl http://localhost:8080/api/v1/health

# Test RAG service
curl http://localhost:8001/health

# Test Ollama
ollama list
```

## ğŸ”§ ConfiguraciÃ³n

Crea `.env` en la raÃ­z (opcional, solo para fine-tuning):

```env
# Solo necesario si vas a entrenar nuevos modelos
KAGGLE_USERNAME=tu_usuario
KAGGLE_KEY=tu_api_key
```

## ğŸš¨ Troubleshooting

### "GROQ_API_KEY not set"

Edita `.env` y agrega tu API key de https://console.groq.com/keys

### "Cannot connect to backend"

Verifica que Docker estÃ© corriendo:
```bash
docker-compose ps
```

### "RAG service not responding"

```bash
# Reinstalar dependencias
cd backend/services
pip install -r requirements.txt
python rag_service.py
```

### Cambiar a Ollama local

```bash
# 1. Instalar Ollama
# 2. Descargar modelo
ollama pull llama3.2:3b

# 3. Editar .env
LLM_PROVIDER=ollama
```

## ğŸ“ TecnologÃ­as

- **Backend:** Go 1.23, Fiber, GORM
- **Frontend:** HTML5, JavaScript, Chart.js
- **LLM:** Groq API (Llama 3.2 3B) o Ollama local
- **RAG:** ChromaDB, sentence-transformers
- **Infraestructura:** Docker, PostgreSQL, MinIO
- **Fine-tuning:** Kaggle Notebooks, Unsloth (opcional)

## ğŸ“Š Modelo

- **Base:** Llama 3.2 3B Instruct
- **Provider:** Groq API (cloud) o Ollama (local)
- **Fine-tuning:** System prompts especializados para contratos legales
- **Mejora:** +25-30% precisiÃ³n vs modelo base sin especializaciÃ³n

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

MIT License - ver [LICENSE](LICENSE)

## ğŸ†˜ Soporte

- **Issues:** GitHub Issues
- **DocumentaciÃ³n:** Ver carpeta `docs/`
- **Email:** [tu-email]

---

**Nota:** LexAnalyzer usa Groq API (cloud) por defecto. No necesitas descargar modelos ni tener GPU.
