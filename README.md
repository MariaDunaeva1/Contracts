# ‚öñÔ∏è LexAnalyzer - Sistema de An√°lisis de Contratos Legales

Sistema completo para an√°lisis inteligente de contratos legales usando grandes modelos de lenguaje (LLMs), Generaci√≥n Aumentada por Recuperaci√≥n (RAG) y Agentes especializados.

## üéØ ¬øQu√© hace este sistema?

Analiza contratos legales autom√°ticamente y extrae de forma estructurada:
- ‚úÖ Cl√°usulas principales
- ‚ö†Ô∏è Riesgos legales y penalizaciones
- üìä Obligaciones de las partes
- üîç Comparaci√≥n con contratos est√°ndar
- üí° Recomendaciones de negociaci√≥n

## üß† Arquitectura

```text
Usuario ‚Üí Frontend (Web) ‚Üí Backend (Go) ‚Üí RAG Service (Python/FastAPI) ‚Üí Groq / Ollama (LLM)
                                                   ‚Üì
                                         ChromaDB (Vector Store)
                                                   ‚Üì
                                        4 Agentes Especializados
```

El sistema ahora incluye una **Base de Conocimiento (Knowledge Base)** que soporta la ingesta nativa de m√∫ltiples formatos de documentos legales para enriquecer el contexto del an√°lisis: `.pdf`, `.docx`, `.txt`, `.csv`, `.md` y `.json/.jsonl`.

## ‚ö° Quick Start

Aseg√∫rate de tener Docker y Docker Compose instalados.

1. **Configurar API (Importante)**
   Crea un archivo `.env` en la ra√≠z del proyecto y a√±ade tu API Key de Groq (el proveedor recomendado por su extrema velocidad y gratuidad):
   ```env
   GROQ_API_KEY=tu_api_key_aqui
   LLM_PROVIDER=groq
   ```

2. **Lanzar todo el ecosistema**
   Todos los microservicios est√°n orquestados; basta con un solo comando:
   ```bash
   docker-compose up --build
   ```
   *(Nota: La primera vez tardar√° varios minutos en descargar los modelos de embeddings de Python).*

3. **Acceder a la Interfaz**
   Abre en tu navegador: http://localhost:3000

## üîë Conceptos Importantes

### ‚úÖ Selector de Modelos Din√°mico
La interfaz de *Contract Analysis* incluye un men√∫ desplegable que lista din√°micamente los modelos disponibles conectados al sistema, divididos en:
- **Modelos Base:** Modelos fundacionales listos para uso r√°pido (ej. Llama 3).
- **Modelos Fine-tuned:** Modelos reentrenados y especializados en el m√≥dulo de *Training* que han finalizado su aprendizaje con √©xito.

### ‚òÅÔ∏è Groq vs Ollama
**Groq (Recomendado - Por defecto):**
- API en la nube gratuita. Inferencia ultra-r√°pida (500+ tokens/segundo).
- No requiere hardware local sofisticado.

**Ollama (Opcional - Local):**
- 100% privado y offline.
- Requiere tener el modelo descargado localmente (`ollama pull llama3.2`) y cambiar en `.env`: `LLM_PROVIDER=ollama`.

### üóÑÔ∏è Almacenamiento y Archivos (MinIO)
LexAnalyzer utiliza **MinIO** (servidor de almacenamiento de objetos compatible con Amazon S3) dentro de Docker para resguardar todos los documentos originales subidos al sistema (`.pdf`, `.docx`, etc.).
- **Consola Web (UI):** Puedes explorar los archivos en crudo accediendo a `http://localhost:9001` (Usuario: `minioadmin` / Contrase√±a: `minioadmin`).
- **Funcionamiento:** El Backend de Go crea autom√°ticamente el bucket necesario y sube los archivos de los usuarios. El microservicio RAG (Python) posteriormente descarga temporalmente fragmentos de estos archivos desde MinIO cuando necesita analizarlos para buscar cl√°usulas.

### üéØ Fine-Tuning Integrado (Kaggle)
El sistema incluye un **Pipeline de Entrenamiento (Fine-Tuning)** completo gestionado desde la interfaz web, sin necesidad de tocar c√≥digo:
1. **Sube tus propios Datasets:** Formato `.json` o `.jsonl` en la secci√≥n *Knowledge Base*.
2. **Lanza un Trabajo (Job):** Selecciona un modelo base, tu dataset, y haz clic en *Start Fine-Tuning*.
3. **Automatizaci√≥n en Kaggle:** El Backend de Go de LexAnalyzer se conecta autom√°ticamente con la API de Kaggle, levanta un cuaderno jupyter temporal con aceleraci√≥n GPU (T4x2 gratuitas) y comienza a entrenar tu modelo usando t√©cnicas de parametrizaci√≥n eficiente (LoRA / Unsloth).
4. **Despliegue Inmediato:** Cuando Kaggle termina, el modelo entrenado se registra en el sistema y aparece autom√°ticamente en el **Selector de Modelos** de la interfaz para poder usarlo en tus pr√≥ximos an√°lisis de contratos.

## üõ†Ô∏è Servicios Activos

Al levantar el sistema, se despliegan autom√°ticamente los siguientes microservicios internos:

| Servicio | Puerto | Descripci√≥n |
|----------|--------|-------------|
| Frontend | 3000 | Interfaz web HTML/JS servida por Nginx |
| Backend API | 8080 | API REST ultra-r√°pida desarrollada en Go |
| RAG Service | 8001 | Motor de IA en Python (FastAPI, Langchain, ChromaDB) |
| PostgreSQL | 5432 | Base de datos relacional para metadatos |
| MinIO | 9000 | Object Storage (clon S3) para almacenar los PDFs y documentos |

## üß™ Estructura B√°sica del Proyecto

```
contracts/
‚îú‚îÄ‚îÄ backend/                    # API Go (Gesti√≥n de base de datos y archivos)
‚îÇ   ‚îî‚îÄ‚îÄ services/               # Motor de IA en Python (RAG, Agentes, embeddings)
‚îú‚îÄ‚îÄ frontend/                   # Interfaz de Usuario (HTML, Vanilla JS, CSS)
‚îú‚îÄ‚îÄ chroma_db/                  # Base de datos vectorial persistente
‚îú‚îÄ‚îÄ data/                       # Datasets de ejemplo
‚îî‚îÄ‚îÄ docker-compose.yml          # Orquestador
```

## üö® Troubleshooting

- **Error "Failed to load" o "Cannot connection to backend":** Verifica que los servicios de Docker se hayan levantado completamente sin errores de memoria (sobre todo el RAG service). Aseg√∫rate de acceder a trav√©s del puerto `3000`.
- **An√°lisis muy lento:** Si est√°s usando Ollama y no tienes GPU dedicada, el an√°lisis de grandes contratos puede llevar varios minutos. P√°sate a Groq configurando `.env`.
- **Despliegue en la nube:** El proyecto cuenta con un `docker-compose.prod.yml` optimizado para servidores como Oracle Cloud (compatible con ARM Ampere A1 de 24GB).

## üìÑ Licencia

MIT License
