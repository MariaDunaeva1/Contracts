{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Fine-Tuning LLM con Unsloth + LoRA\n",
                "\n",
                "**Modelo**: unsloth/Llama-3.2-3B-Instruct (4-bit quantized)\n",
                "**Dataset**: sentiment_500.json (upload como Kaggle Dataset)\n",
                "**MÃ©todo**: QLoRA (4-bit) con Unsloth para 2x velocidad\n",
                "\n",
                "### Instrucciones:\n",
                "1. Sube `sentiment_500.json` como un Kaggle Dataset\n",
                "2. AÃ±ade ese dataset a este notebook (botÃ³n '+Add Input')\n",
                "3. Selecciona GPU T4 x2 como acelerador\n",
                "4. Activa Internet\n",
                "5. Ejecuta todas las celdas"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Setup & InstalaciÃ³n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes triton\n",
                "!pip install minio\n",
                "print('âœ… InstalaciÃ³n completada')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Imports & Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import json\n",
                "import os\n",
                "import time\n",
                "from datetime import datetime\n",
                "from datasets import Dataset\n",
                "from unsloth import FastLanguageModel\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ðŸ“ CONFIGURACIÃ“N - Edita estos valores\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # Modelo preoptimizado 4-bit\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "EPOCHS = 3\n",
                "BATCH_SIZE = 2\n",
                "GRADIENT_ACCUMULATION = 4\n",
                "LEARNING_RATE = 2e-4\n",
                "LORA_R = 16\n",
                "LORA_ALPHA = 16\n",
                "\n",
                "# Ruta al dataset (ajusta segÃºn cÃ³mo lo subas)\n",
                "# Si lo subes como Kaggle Dataset, la ruta serÃ¡ algo como:\n",
                "# /kaggle/input/{tu-usuario}-{nombre-dataset}/{archivo}.json\n",
                "DATASET_PATH = \"/kaggle/input/\"  # <-- AJUSTA ESTA RUTA\n",
                "\n",
                "# Buscar el JSON automÃ¡ticamente\n",
                "for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
                "    for f in files:\n",
                "        if f.endswith(\".json\"):\n",
                "            DATASET_PATH = os.path.join(root, f)\n",
                "            break\n",
                "\n",
                "print(f\"ðŸ“‚ Dataset encontrado: {DATASET_PATH}\")\n",
                "print(f\"ðŸ§  Modelo: {MODEL_NAME}\")\n",
                "print(f\"ðŸ”§ LoRA r={LORA_R}, alpha={LORA_ALPHA}\")\n",
                "print(f\"ðŸ“Š Epochs: {EPOCHS}, Batch: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
                "print(f\"ðŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
                "print(f\"ðŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\" if torch.cuda.is_available() else '')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Cargar Modelo (4-bit Quantized)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_NAME,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=None,  # Auto-detect: float16 para T4, bfloat16 para Ampere+\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "print(f\"âœ… Modelo cargado: {MODEL_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Aplicar LoRA Adapters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=LORA_R,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=LORA_ALPHA,\n",
                "    lora_dropout=0,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=3407,\n",
                "    use_rslora=False,\n",
                "    loftq_config=None,\n",
                ")\n",
                "\n",
                "# Contar parÃ¡metros entrenables\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "total = sum(p.numel() for p in model.parameters())\n",
                "print(f\"âœ… LoRA aplicado: {trainable:,} params entrenables / {total:,} total ({100*trainable/total:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Preparar Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Cargar JSON\n",
                "with open(DATASET_PATH, 'r') as f:\n",
                "    raw_data = json.load(f)\n",
                "\n",
                "print(f\"ðŸ“Š Dataset cargado: {len(raw_data)} ejemplos\")\n",
                "\n",
                "# Mostrar distribuciÃ³n de clases\n",
                "from collections import Counter\n",
                "label_dist = Counter(item['label'] for item in raw_data)\n",
                "print(f\"ðŸ“Š DistribuciÃ³n: {dict(label_dist)}\")\n",
                "\n",
                "# Formatear como instrucciones para el modelo\n",
                "PROMPT_TEMPLATE = \"\"\"### Instruction:\n",
                "Classify the sentiment of the following text.\n",
                "\n",
                "### Input:\n",
                "{text}\n",
                "\n",
                "### Response:\n",
                "{label}\"\"\"\n",
                "\n",
                "formatted_data = []\n",
                "for item in raw_data:\n",
                "    formatted_data.append({\n",
                "        \"text\": PROMPT_TEMPLATE.format(text=item[\"text\"], label=item[\"label\"])\n",
                "    })\n",
                "\n",
                "dataset = Dataset.from_list(formatted_data)\n",
                "print(f\"âœ… Dataset formateado: {len(dataset)} ejemplos\")\n",
                "print(f\"\\nðŸ“ Ejemplo:\")\n",
                "print(dataset[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Configurar Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dataset_num_proc=2,\n",
                "    packing=False,\n",
                "    args=TrainingArguments(\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
                "        warmup_steps=5,\n",
                "        num_train_epochs=EPOCHS,\n",
                "        learning_rate=LEARNING_RATE,\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        logging_steps=1,\n",
                "        optim=\"adamw_8bit\",\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"linear\",\n",
                "        seed=3407,\n",
                "        output_dir=\"outputs\",\n",
                "        save_strategy=\"epoch\",\n",
                "        report_to=\"none\",\n",
                "    ),\n",
                ")\n",
                "print(\"âœ… Trainer configurado\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: ðŸ”¥ Entrenar"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "start_time = time.time()\n",
                "print(f\"ðŸ‹ï¸ Entrenamiento iniciado: {datetime.now().strftime('%H:%M:%S')}\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "trainer_stats = trainer.train()\n",
                "\n",
                "elapsed = time.time() - start_time\n",
                "print(\"=\" * 50)\n",
                "print(f\"âœ… Entrenamiento completado en {elapsed/60:.1f} minutos\")\n",
                "print(f\"ðŸ“‰ Training Loss Final: {trainer_stats.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: EvaluaciÃ³n RÃ¡pida"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Probar el modelo fine-tuned\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "test_texts = [\n",
                "    \"I absolutely love this product! Best purchase ever.\",\n",
                "    \"Terrible experience. Complete waste of money.\",\n",
                "    \"It was okay, nothing special but does the job.\",\n",
                "    \"Amazing quality and fast delivery, highly recommended!\",\n",
                "    \"Broken on arrival, very disappointed.\",\n",
                "]\n",
                "\n",
                "print(\"ðŸ§ª EvaluaciÃ³n con textos de prueba:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for text in test_texts:\n",
                "    prompt = f\"\"\"### Instruction:\n",
                "Classify the sentiment of the following text.\n",
                "\n",
                "### Input:\n",
                "{text}\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    outputs = model.generate(**inputs, max_new_tokens=10, temperature=0.1)\n",
                "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Extraer solo la respuesta\n",
                "    response = result.split(\"### Response:\")[-1].strip().split(\"\\n\")[0]\n",
                "    print(f\"  Input: {text[:50]}...\")\n",
                "    print(f\"  Pred:  {response}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Exportar LoRA Adapters (~20MB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "LORA_OUTPUT = \"/kaggle/working/lora_model\"\n",
                "model.save_pretrained(LORA_OUTPUT)\n",
                "tokenizer.save_pretrained(LORA_OUTPUT)\n",
                "\n",
                "# Calcular tamaÃ±o\n",
                "lora_size = sum(os.path.getsize(os.path.join(LORA_OUTPUT, f)) for f in os.listdir(LORA_OUTPUT))\n",
                "print(f\"âœ… LoRA adapters guardados en: {LORA_OUTPUT}\")\n",
                "print(f\"ðŸ’¾ TamaÃ±o: {lora_size / 1e6:.1f} MB\")\n",
                "print(f\"ðŸ“ Archivos: {os.listdir(LORA_OUTPUT)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Exportar GGUF (para uso local con llama.cpp/Ollama)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Exportar a GGUF q4_k_m (buen balance calidad/tamaÃ±o)\n",
                "GGUF_OUTPUT = \"/kaggle/working/gguf_model\"\n",
                "\n",
                "try:\n",
                "    model.save_pretrained_gguf(\n",
                "        GGUF_OUTPUT,\n",
                "        tokenizer,\n",
                "        quantization_method=\"q4_k_m\",\n",
                "    )\n",
                "    \n",
                "    # Mostrar archivos generados\n",
                "    for f in os.listdir(GGUF_OUTPUT):\n",
                "        size = os.path.getsize(os.path.join(GGUF_OUTPUT, f)) / 1e9\n",
                "        print(f\"  {f}: {size:.2f} GB\")\n",
                "    print(f\"âœ… GGUF exportado a: {GGUF_OUTPUT}\")\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ GGUF export fallÃ³ (puede requerir mÃ¡s RAM): {e}\")\n",
                "    print(\"Los LoRA adapters siguen disponibles en /kaggle/working/lora_model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 11: Guardar MÃ©tricas & Resumen"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# Guardar mÃ©tricas del entrenamiento\n",
                "metrics = {\n",
                "    \"model\": MODEL_NAME,\n",
                "    \"dataset_path\": DATASET_PATH,\n",
                "    \"dataset_size\": len(raw_data),\n",
                "    \"epochs\": EPOCHS,\n",
                "    \"batch_size\": BATCH_SIZE,\n",
                "    \"learning_rate\": LEARNING_RATE,\n",
                "    \"lora_r\": LORA_R,\n",
                "    \"lora_alpha\": LORA_ALPHA,\n",
                "    \"training_loss\": trainer_stats.training_loss,\n",
                "    \"training_time_minutes\": elapsed / 60,\n",
                "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
                "    \"timestamp\": datetime.now().isoformat(),\n",
                "    \"log_history\": trainer_stats.metrics,\n",
                "}\n",
                "\n",
                "with open(\"/kaggle/working/training_metrics.json\", \"w\") as f:\n",
                "    json.dump(metrics, f, indent=2, default=str)\n",
                "\n",
                "print(\"ðŸ“Š Resumen del Entrenamiento\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"  Modelo:          {MODEL_NAME}\")\n",
                "print(f\"  Dataset:         {len(raw_data)} ejemplos\")\n",
                "print(f\"  Epochs:          {EPOCHS}\")\n",
                "print(f\"  Loss Final:      {trainer_stats.training_loss:.4f}\")\n",
                "print(f\"  Tiempo:          {elapsed/60:.1f} min\")\n",
                "print(f\"  GPU:             {torch.cuda.get_device_name(0)}\")\n",
                "print(\"=\" * 40)\n",
                "print(\"\\nðŸ“ Archivos de salida en /kaggle/working/:\")\n",
                "print(\"  - lora_model/          (LoRA adapters)\")\n",
                "print(\"  - gguf_model/          (GGUF para Ollama)\")\n",
                "print(\"  - training_metrics.json\")\n",
                "print(\"\\nðŸŽ‰ Â¡Fine-tuning completado! Descarga los archivos desde la pestaÃ±a Output.\")"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [],
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}