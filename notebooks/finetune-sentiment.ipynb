{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14866641,"sourceType":"datasetVersion","datasetId":9510239}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸš€ Fine-Tuning LLM con Unsloth + LoRA\n\n**Modelo**: unsloth/Llama-3.2-3B-Instruct (4-bit quantized)\n**Dataset**: ledgar_finetune_train.json (~60K ejemplos del LEDGAR dataset)\n**MÃ©todo**: QLoRA (4-bit) con Unsloth para 2x velocidad\n\n### Instrucciones:\n1. Sube `sentiment_500.json` como un Kaggle Dataset\n2. AÃ±ade ese dataset a este notebook (botÃ³n '+Add Input')\n3. Selecciona GPU T4 x2 como acelerador\n4. Activa Internet\n5. Ejecuta todas las celdas","metadata":{}},{"cell_type":"code","source":"import torch\nassert torch.cuda.is_available(), \"âŒ GPU NO detectada. Ve a Settings â†’ Accelerator â†’ GPU T4 x2\"\ntorch.cuda.set_device(0)\nprint(f\"âœ… GPU activa: {torch.cuda.get_device_name(0)}\")\nprint(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:32:43.031335Z","iopub.execute_input":"2026-02-17T15:32:43.031629Z","iopub.status.idle":"2026-02-17T15:32:46.776432Z","shell.execute_reply.started":"2026-02-17T15:32:43.031603Z","shell.execute_reply":"2026-02-17T15:32:46.775420Z"}},"outputs":[{"name":"stdout","text":"âœ… GPU activa: Tesla T4\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1080403993.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… GPU activa: {torch.cuda.get_device_name(0)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'torch._C._CudaDeviceProperties' object has no attribute 'total_mem'"],"ename":"AttributeError","evalue":"'torch._C._CudaDeviceProperties' object has no attribute 'total_mem'","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"## Cell 1: Setup & InstalaciÃ³n","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install --upgrade pip\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --upgrade \"transformers>=4.46\" \"trl>=0.12.0\" peft accelerate bitsandbytes\n!pip install minio\nprint('âœ… InstalaciÃ³n completada (Versiones Modernas)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:32:46.829095Z","iopub.execute_input":"2026-02-17T15:32:46.829807Z","iopub.status.idle":"2026-02-17T15:33:24.358194Z","shell.execute_reply.started":"2026-02-17T15:32:46.829776Z","shell.execute_reply":"2026-02-17T15:33:24.357384Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Cell 2: Imports & Config","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom datasets import Dataset\nfrom unsloth import FastLanguageModel\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“ CONFIGURACIÃ“N - Edita estos valores\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nMODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # Modelo preoptimizado 4-bit\nMAX_SEQ_LENGTH = 1024\nEPOCHS = 1\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION = 4\nLEARNING_RATE = 2e-4\nLORA_R = 16\nLORA_ALPHA = 16\n\n# Ruta al dataset (ajusta segÃºn cÃ³mo lo subas)\n# Si lo subes como Kaggle Dataset, la ruta serÃ¡ algo como:\n# /kaggle/input/{tu-usuario}-{nombre-dataset}/{archivo}.json\nDATASET_PATH = \"/kaggle/input/datasets/maradg/ledgar/ledgar_finetune_train.json\"\nVAL_PATH = \"/kaggle/input/datasets/maradg/ledgar/ledgar_finetune_val.json\"\nTEST_PATH = \"/kaggle/input/datasets/maradg/ledgar/ledgar_finetune_test.json\"\n\n# Auto-find\nfor root, dirs, files in os.walk(\"/kaggle/input/\"):\n    for f in files:\n        if f == \"ledgar_finetune_train.json\":\n            DATASET_PATH = os.path.join(root, f)\n        elif f == \"ledgar_finetune_val.json\":\n            VAL_PATH = os.path.join(root, f)\n        elif f == \"ledgar_finetune_test.json\":\n            TEST_PATH = os.path.join(root, f)\n\nprint(f\"ğŸ“‚ Dataset encontrado: {DATASET_PATH}\")\nprint(f\"ğŸ§  Modelo: {MODEL_NAME}\")\nprint(f\"ğŸ”§ LoRA r={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"ğŸ“Š Epochs: {EPOCHS}, Batch: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\nprint(f\"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\nprint(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if torch.cuda.is_available() else '')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:34:04.441445Z","iopub.execute_input":"2026-02-17T15:34:04.441801Z","iopub.status.idle":"2026-02-17T15:34:04.452107Z","shell.execute_reply.started":"2026-02-17T15:34:04.441750Z","shell.execute_reply":"2026-02-17T15:34:04.451269Z"}},"outputs":[{"name":"stdout","text":"ğŸ“‚ Dataset encontrado: /kaggle/input/datasets/maradg/ledgar/ledgar_finetune_train.json\nğŸ§  Modelo: unsloth/Llama-3.2-3B-Instruct-bnb-4bit\nğŸ”§ LoRA r=16, alpha=16\nğŸ“Š Epochs: 1, Batch: 4, LR: 0.0002\nğŸ–¥ï¸ GPU: Tesla T4\nğŸ’¾ VRAM: 15.6 GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Cell 3: Cargar Modelo (4-bit Quantized)","metadata":{}},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=MODEL_NAME,\n    max_seq_length=MAX_SEQ_LENGTH,\n    dtype=None,  # Auto-detect: float16 para T4, bfloat16 para Ampere+\n    load_in_4bit=True,\n)\nprint(f\"âœ… Modelo cargado: {MODEL_NAME}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:34:14.807417Z","iopub.execute_input":"2026-02-17T15:34:14.808261Z","iopub.status.idle":"2026-02-17T15:34:34.365374Z","shell.execute_reply.started":"2026-02-17T15:34:14.808211Z","shell.execute_reply":"2026-02-17T15:34:34.364663Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 5.2.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4da5dbbb250743b5942b7621628dd820"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/254 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d169417f51d045c8910b7b609cc9a7d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51239b0b46524dd999fa08963e10fe8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc5bc67f91c4c148685dd92a3ff0a5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0973fa00101c4f1ba887b45fd7db5df4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0753a68a834b71894be8d067411f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a4c3757fd44e7dadcd548e26a7bdcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4abfc056d21c4a15b3c32ac5dcd4c6d2"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Will load unsloth/llama-3.2-3b-instruct-bnb-4bit as a legacy tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Modelo cargado: unsloth/Llama-3.2-3B-Instruct-bnb-4bit\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Cell 4: Aplicar LoRA Adapters","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=LORA_R,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Contar parÃ¡metros entrenables\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"âœ… LoRA aplicado: {trainable:,} params entrenables / {total:,} total ({100*trainable/total:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:35:29.865886Z","iopub.execute_input":"2026-02-17T15:35:29.866641Z","iopub.status.idle":"2026-02-17T15:35:34.454136Z","shell.execute_reply.started":"2026-02-17T15:35:29.866590Z","shell.execute_reply":"2026-02-17T15:35:34.453197Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2026.2.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"âœ… LoRA aplicado: 24,313,856 params entrenables / 1,827,777,536 total (1.33%)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Cell 5: Preparar Dataset","metadata":{}},{"cell_type":"code","source":"with open(DATASET_PATH, 'r') as f:\n    raw_data = json.load(f)\nwith open(VAL_PATH, 'r') as f:\n    val_data = json.load(f)\n\nprint(f\"ğŸ“Š Train: {len(raw_data)} ejemplos\")\nprint(f\"ğŸ“Š Val: {len(val_data)} ejemplos\")\n\ndef format_messages(data):\n    formatted = []\n    for item in data:\n        msgs = item[\"messages\"]\n        user_msg = \"\"\n        assistant_msg = \"\"\n        for m in msgs:\n            if m[\"role\"] == \"user\":\n                user_msg = m[\"content\"]\n            elif m[\"role\"] == \"assistant\":\n                assistant_msg = m[\"content\"]\n        text = f\"### Instruction:\\n{user_msg}\\n\\n### Response:\\n{assistant_msg}\"\n        formatted.append({\"text\": text})\n    return formatted\n\ndataset = Dataset.from_list(format_messages(raw_data))\nval_dataset = Dataset.from_list(format_messages(val_data))\n\nprint(f\"âœ… Train formateado: {len(dataset)} | Val formateado: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:35:40.629484Z","iopub.execute_input":"2026-02-17T15:35:40.630109Z","iopub.status.idle":"2026-02-17T15:35:42.800938Z","shell.execute_reply.started":"2026-02-17T15:35:40.630056Z","shell.execute_reply":"2026-02-17T15:35:42.800302Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š Train: 60000 ejemplos\nğŸ“Š Val: 10000 ejemplos\nâœ… Train formateado: 60000 | Val formateado: 10000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Cell 6: Configurar Trainer","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom transformers import TrainingArguments\n\n# Importante: SFTConfig reemplaza a parte de los argumentos antiguos\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    eval_dataset=val_dataset,  # â† Ya podemos usar esto\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        max_seq_length=MAX_SEQ_LENGTH,\n        per_device_train_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n        warmup_steps=5,\n        num_train_epochs=EPOCHS,\n        learning_rate=LEARNING_RATE,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=50,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        save_strategy=\"steps\",     # Estrategia de guardado\n        save_steps=500,\n        eval_strategy=\"steps\",     # Estrategia de evaluaciÃ³n\n        eval_steps=500,\n        report_to=\"none\",\n        packing=False,\n    ),\n)\nprint(\"âœ… Trainer configurado (Modern Stack)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:35:46.344423Z","iopub.execute_input":"2026-02-17T15:35:46.345202Z","iopub.status.idle":"2026-02-17T15:36:12.195997Z","shell.execute_reply.started":"2026-02-17T15:35:46.345155Z","shell.execute_reply":"2026-02-17T15:36:12.195319Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f37866b49c244d63b1a70cf94ceec919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd03b32cfe643ea93138834f117b0c2"}},"metadata":{}},{"name":"stdout","text":"ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\nâœ… Trainer configurado (Modern Stack)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Cell 7: ğŸ”¥ Entrenar","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nprint(f\"ğŸ‹ï¸ Entrenamiento iniciado: {datetime.now().strftime('%H:%M:%S')}\")\nprint(\"=\" * 50)\n\ntrainer_stats = trainer.train()\n\nelapsed = time.time() - start_time\nprint(\"=\" * 50)\nprint(f\"âœ… Entrenamiento completado en {elapsed/60:.1f} minutos\")\nprint(f\"ğŸ“‰ Training Loss Final: {trainer_stats.training_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T15:36:23.414467Z","iopub.execute_input":"2026-02-17T15:36:23.415197Z","iopub.status.idle":"2026-02-17T22:21:17.677566Z","shell.execute_reply.started":"2026-02-17T15:36:23.415143Z","shell.execute_reply":"2026-02-17T22:21:17.676897Z"}},"outputs":[{"name":"stdout","text":"ğŸ‹ï¸ Entrenamiento iniciado: 15:36:23\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 60,000 | Num Epochs = 1 | Total steps = 3,750\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3750/3750 6:44:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.252988</td>\n      <td>1.230173</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.203507</td>\n      <td>1.160746</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.132081</td>\n      <td>1.115132</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.106642</td>\n      <td>1.082388</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.080255</td>\n      <td>1.058102</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.077807</td>\n      <td>1.038220</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.058463</td>\n      <td>1.026372</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nâœ… Entrenamiento completado en 404.9 minutos\nğŸ“‰ Training Loss Final: 1.1487\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Cell 8: EvaluaciÃ³n RÃ¡pida","metadata":{}},{"cell_type":"code","source":"import random\n\nFastLanguageModel.for_inference(model)\n\n# Cargar test set real\nwith open(TEST_PATH, 'r') as f:\n    test_data = json.load(f)\n\nprint(f\"ğŸ§ª Test set: {len(test_data)} ejemplos\")\n\n# Evaluar sobre una muestra aleatoria\nsample_size = 50\nsample = random.sample(test_data, min(sample_size, len(test_data)))\n\ncorrect = 0\ntotal = len(sample)\n\nprint(f\"ğŸ“Š Evaluando {total} ejemplos del test set...\")\nprint(\"=\" * 70)\n\nfor item in sample:\n    msgs = item[\"messages\"]\n    clause = [m[\"content\"] for m in msgs if m[\"role\"] == \"user\"][0]\n    expected = [m[\"content\"] for m in msgs if m[\"role\"] == \"assistant\"][0]\n\n    prompt = f\"### Instruction:\\n{clause}\\n\\n### Response:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=20, temperature=0.1)\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = result.split(\"### Response:\")[-1].strip().split(\"\\n\")[0]\n\n    match = expected.lower() == response.lower()\n    if match:\n        correct += 1\n    icon = \"âœ…\" if match else \"âŒ\"\n    print(f\"  {icon} Esperado: {expected:25s} | PredicciÃ³n: {response}\")\n\nprint(\"=\" * 70)\nprint(f\"ğŸ“Š Accuracy: {correct}/{total} ({100*correct/total:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T22:27:21.074486Z","iopub.execute_input":"2026-02-17T22:27:21.074946Z","iopub.status.idle":"2026-02-17T22:28:21.498482Z","shell.execute_reply.started":"2026-02-17T22:27:21.074908Z","shell.execute_reply":"2026-02-17T22:28:21.497863Z"}},"outputs":[{"name":"stdout","text":"ğŸ§ª Test set: 10000 ejemplos\nğŸ“Š Evaluando 50 ejemplos del test set...\n======================================================================\n  âŒ Esperado: Entire Agreements         | PredicciÃ³n: Misc\n  âŒ Esperado: Waiver Of Jury Trials     | PredicciÃ³n: Waivers.\n  âŒ Esperado: Death                     | PredicciÃ³n: Death Benefits\n  âŒ Esperado: Change In Control         | PredicciÃ³n: Change In Control.\n  âœ… Esperado: Assignments               | PredicciÃ³n: Assignments\n  âœ… Esperado: Notices                   | PredicciÃ³n: Notices\n  âŒ Esperado: Enforceability            | PredicciÃ³n: Misc\n  âŒ Esperado: Disclosures               | PredicciÃ³n: Records.\n  âŒ Esperado: Binding Effects           | PredicciÃ³n: Assignments\n  âŒ Esperado: Effective Dates           | PredicciÃ³n: Waivers\n  âŒ Esperado: Headings                  | PredicciÃ³n: \n  âŒ Esperado: Taxes                     | PredicciÃ³n: \n  âŒ Esperado: Change In Control         | PredicciÃ³n: Change\n  âœ… Esperado: Taxes                     | PredicciÃ³n: Taxes\n  âŒ Esperado: Counterparts              | PredicciÃ³n: Counterparts. ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###\n  âœ… Esperado: Counterparts              | PredicciÃ³n: Counterparts\n  âœ… Esperado: Publicity                 | PredicciÃ³n: Publicity\n  âŒ Esperado: Existence                 | PredicciÃ³n: \n  âŒ Esperado: Closings                  | PredicciÃ³n: Payments\n  âŒ Esperado: Tax Withholdings          | PredicciÃ³n: \n  âœ… Esperado: Confidentiality           | PredicciÃ³n: Confidentiality\n  âŒ Esperado: Tax Withholdings          | PredicciÃ³n: \n  âœ… Esperado: Subsidiaries              | PredicciÃ³n: Subsidiaries\n  âœ… Esperado: Interests                 | PredicciÃ³n: Interests\n  âŒ Esperado: Sales                     | PredicciÃ³n: Sales And Leases\n  âŒ Esperado: Notices                   | PredicciÃ³n: General\n  âŒ Esperado: Forfeitures               | PredicciÃ³n: Assign\n  âœ… Esperado: Notices                   | PredicciÃ³n: Notices\n  âŒ Esperado: Further Assurances        | PredicciÃ³n: Further Assurances.\n  âŒ Esperado: Governing Laws            | PredicciÃ³n: Remedies\n  âŒ Esperado: Miscellaneous             | PredicciÃ³n: Governing Laws\n  âŒ Esperado: Terminations              | PredicciÃ³n: Terminations.\n  âŒ Esperado: Applicable Laws           | PredicciÃ³n: Governing Laws\n  âŒ Esperado: No Defaults               | PredicciÃ³n: \n  âŒ Esperado: Survival                  | PredicciÃ³n: Ag\n  âŒ Esperado: Severability              | PredicciÃ³n: Interpret\n  âœ… Esperado: Use Of Proceeds           | PredicciÃ³n: Use Of Proceeds\n  âŒ Esperado: Indemnifications          | PredicciÃ³n: Indemnity.\n  âœ… Esperado: Insurances                | PredicciÃ³n: Insurances\n  âŒ Esperado: Miscellaneous             | PredicciÃ³n: Governing Laws\n  âŒ Esperado: Fees                      | PredicciÃ³n: Expenses\n  âŒ Esperado: Modifications             | PredicciÃ³n: Amendments.\n  âŒ Esperado: Fees                      | PredicciÃ³n: \n  âŒ Esperado: Construction              | PredicciÃ³n: \n  âŒ Esperado: Governing Laws            | PredicciÃ³n: Miscellaneous\n  âŒ Esperado: Governing Laws            | PredicciÃ³n: Consent To Jurisdiction\n  âŒ Esperado: Construction              | PredicciÃ³n: Construction.\n  âŒ Esperado: Intellectual Property     | PredicciÃ³n: Licensing Agreements.\n  âŒ Esperado: Confidentiality           | PredicciÃ³n: Confidentiality.\n  âŒ Esperado: Notices                   | PredicciÃ³n: General\n======================================================================\nğŸ“Š Accuracy: 11/50 (22.0%)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Cell 9: Exportar LoRA Adapters (~20MB)","metadata":{}},{"cell_type":"code","source":"LORA_OUTPUT = \"/kaggle/working/lora_model\"\nmodel.save_pretrained(LORA_OUTPUT)\ntokenizer.save_pretrained(LORA_OUTPUT)\n\n# Calcular tamaÃ±o\nlora_size = sum(os.path.getsize(os.path.join(LORA_OUTPUT, f)) for f in os.listdir(LORA_OUTPUT))\nprint(f\"âœ… LoRA adapters guardados en: {LORA_OUTPUT}\")\nprint(f\"ğŸ’¾ TamaÃ±o: {lora_size / 1e6:.1f} MB\")\nprint(f\"ğŸ“ Archivos: {os.listdir(LORA_OUTPUT)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T22:28:44.731169Z","iopub.execute_input":"2026-02-17T22:28:44.731488Z","iopub.status.idle":"2026-02-17T22:28:45.486258Z","shell.execute_reply.started":"2026-02-17T22:28:44.731449Z","shell.execute_reply":"2026-02-17T22:28:45.485528Z"}},"outputs":[{"name":"stdout","text":"âœ… LoRA adapters guardados en: /kaggle/working/lora_model\nğŸ’¾ TamaÃ±o: 114.5 MB\nğŸ“ Archivos: ['tokenizer_config.json', 'tokenizer.json', 'README.md', 'adapter_model.safetensors', 'chat_template.jinja', 'adapter_config.json']\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Cell 10: Exportar GGUF (para uso local con llama.cpp/Ollama)","metadata":{}},{"cell_type":"code","source":"# Exportar a GGUF q4_k_m (buen balance calidad/tamaÃ±o)\nGGUF_OUTPUT = \"/kaggle/working/gguf_model\"\n\ntry:\n    model.save_pretrained_gguf(\n        GGUF_OUTPUT,\n        tokenizer,\n        quantization_method=\"q4_k_m\",\n    )\n    \n    # Mostrar archivos generados\n    for f in os.listdir(GGUF_OUTPUT):\n        size = os.path.getsize(os.path.join(GGUF_OUTPUT, f)) / 1e9\n        print(f\"  {f}: {size:.2f} GB\")\n    print(f\"âœ… GGUF exportado a: {GGUF_OUTPUT}\")\nexcept Exception as e:\n    print(f\"âš ï¸ GGUF export fallÃ³ (puede requerir mÃ¡s RAM): {e}\")\n    print(\"Los LoRA adapters siguen disponibles en /kaggle/working/lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T22:28:50.982481Z","iopub.execute_input":"2026-02-17T22:28:50.982952Z","iopub.status.idle":"2026-02-17T22:38:50.144380Z","shell.execute_reply.started":"2026-02-17T22:28:50.982881Z","shell.execute_reply":"2026-02-17T22:38:50.143605Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Merging model weights to 16-bit format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d31cc1430ed46c9bbad6debee43c8e6"}},"metadata":{}},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b0803ff3ce47fea5003168fba4fd20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b4789e2dc14fb2872668a0d3da5b44"}},"metadata":{}},{"name":"stdout","text":"Checking cache directory for required files...\nCache check failed: model-00001-of-00002.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"\nUnsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7084e58f16b4df4800235fb186aa762"}},"metadata":{}},{"name":"stderr","text":"\nUnsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:17<00:17, 17.80s/it]\u001b[A","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5e9f0a0d62449c8e25a811a6895ecf"}},"metadata":{}},{"name":"stderr","text":"\nUnsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.57s/it]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n","output_type":"stream"},{"name":"stderr","text":"\nUnsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\nUnsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:32<00:32, 32.43s/it]\u001b[A\nUnsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:42<00:00, 21.10s/it]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/gguf_model`\nUnsloth: Converting to GGUF format...\n==((====))==  Unsloth: Conversion from HF to GGUF information\n   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n\\        /    [2] Converting GGUF f16 to ['q4_k_m'] might take 10 minutes each.\n \"-____-\"     In total, you will have to wait at least 16 minutes.\n\nUnsloth: Installing llama.cpp. This might take 3 minutes...\nUnsloth: Updating system package directories\nUnsloth: All required system packages already installed!\nUnsloth: Install llama.cpp and building - please wait 1 to 3 minutes\nUnsloth: Cloning llama.cpp repository\nUnsloth: Install GGUF and other packages\nUnsloth: Successfully installed llama.cpp!\nUnsloth: Preparing converter script...\n","output_type":"stream"},{"name":"stderr","text":"[unsloth_zoo.llama_cpp|WARNING]Unsloth: Qwen2MoE num_experts patch target not found.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: [1] Converting model into f16 GGUF format.\nThis might take 3 minutes...\nUnsloth: Initial conversion completed! Files: ['/kaggle/working/gguf_model_gguf/Llama-3.2-3B-Instruct.F16.gguf']\nUnsloth: [2] Converting GGUF f16 into q4_k_m. This might take 10 minutes...\nUnsloth: Model files cleanup...\nUnsloth: All GGUF conversions completed successfully!\nGenerated files: ['/kaggle/working/gguf_model_gguf/Llama-3.2-3B-Instruct.Q4_K_M.gguf']\nUnsloth: example usage for text only LLMs: llama.cpp/llama-cli --model /kaggle/working/gguf_model_gguf/Llama-3.2-3B-Instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\nUnsloth: Saved Ollama Modelfile to /kaggle/working/gguf_model_gguf/Modelfile\nUnsloth: convert model to ollama format by running - ollama create model_name -f /kaggle/working/gguf_model_gguf/Modelfile\n  .cache: 0.00 GB\n  model-00001-of-00002.safetensors: 4.97 GB\n  tokenizer_config.json: 0.00 GB\n  model-00002-of-00002.safetensors: 1.46 GB\n  tokenizer.json: 0.02 GB\n  config.json: 0.00 GB\n  model.safetensors.index.json: 0.00 GB\n  chat_template.jinja: 0.00 GB\nâœ… GGUF exportado a: /kaggle/working/gguf_model\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Cell 11: Guardar MÃ©tricas & Resumen","metadata":{}},{"cell_type":"code","source":"# Guardar mÃ©tricas del entrenamiento\nmetrics = {\n    \"model\": MODEL_NAME,\n    \"dataset_path\": DATASET_PATH,\n    \"dataset_size\": len(raw_data),\n    \"epochs\": EPOCHS,\n    \"batch_size\": BATCH_SIZE,\n    \"learning_rate\": LEARNING_RATE,\n    \"lora_r\": LORA_R,\n    \"lora_alpha\": LORA_ALPHA,\n    \"training_loss\": trainer_stats.training_loss,\n    \"training_time_minutes\": elapsed / 60,\n    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n    \"timestamp\": datetime.now().isoformat(),\n    \"log_history\": trainer_stats.metrics,\n}\n\nwith open(\"/kaggle/working/training_metrics.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2, default=str)\n\nprint(\"ğŸ“Š Resumen del Entrenamiento\")\nprint(\"=\" * 40)\nprint(f\"  Modelo:          {MODEL_NAME}\")\nprint(f\"  Dataset:         {len(raw_data)} ejemplos\")\nprint(f\"  Epochs:          {EPOCHS}\")\nprint(f\"  Loss Final:      {trainer_stats.training_loss:.4f}\")\nprint(f\"  Tiempo:          {elapsed/60:.1f} min\")\nprint(f\"  GPU:             {torch.cuda.get_device_name(0)}\")\nprint(\"=\" * 40)\nprint(\"\\nğŸ“ Archivos de salida en /kaggle/working/:\")\nprint(\"  - lora_model/          (LoRA adapters)\")\nprint(\"  - gguf_model/          (GGUF para Ollama)\")\nprint(\"  - training_metrics.json\")\nprint(\"\\nğŸ‰ Â¡Fine-tuning completado! Descarga los archivos desde la pestaÃ±a Output.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T22:38:50.155802Z","iopub.execute_input":"2026-02-17T22:38:50.156446Z","iopub.status.idle":"2026-02-17T22:38:50.184487Z","shell.execute_reply.started":"2026-02-17T22:38:50.156374Z","shell.execute_reply":"2026-02-17T22:38:50.183768Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š Resumen del Entrenamiento\n========================================\n  Modelo:          unsloth/Llama-3.2-3B-Instruct-bnb-4bit\n  Dataset:         60000 ejemplos\n  Epochs:          1\n  Loss Final:      1.1487\n  Tiempo:          404.9 min\n  GPU:             Tesla T4\n========================================\n\nğŸ“ Archivos de salida en /kaggle/working/:\n  - lora_model/          (LoRA adapters)\n  - gguf_model/          (GGUF para Ollama)\n  - training_metrics.json\n\nğŸ‰ Â¡Fine-tuning completado! Descarga los archivos desde la pestaÃ±a Output.\n","output_type":"stream"}],"execution_count":26}]}