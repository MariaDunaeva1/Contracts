# ‚úÖ Cambios Realizados - Sistema con Groq API

## Resumen

Se ha actualizado el sistema para:
1. ‚úÖ Permitir elegir entre modelo base y fine-tuned en la UI
2. ‚úÖ Usar Groq API (cloud) por defecto en lugar de Ollama (ahorra 5GB)
3. ‚úÖ Mantener compatibilidad con Ollama local (opcional)

## Archivos Modificados

### Backend Python

1. **backend/services/llm_service.py**
   - Agregado soporte para Groq API
   - Par√°metro `provider` para elegir entre "groq" o "ollama"
   - Par√°metro `use_finetuned` para elegir modelo
   - M√©todo `_complete_groq()` para llamadas a Groq
   - M√©todo `_complete_ollama()` para llamadas a Ollama

2. **backend/services/rag_service.py**
   - Agregado campo `use_finetuned` en `AnalyzeRequest`
   - Detecta provider desde variable de entorno `LLM_PROVIDER`
   - Pasa par√°metro `use_finetuned` al orchestrator

3. **backend/services/agents/base_agent.py**
   - Agregado par√°metro `use_finetuned` en constructor
   - Nuevo m√©todo `complete()` que envuelve `llm.complete()`
   - Pasa autom√°ticamente `use_finetuned` al LLM

4. **backend/services/agents/orchestrator.py**
   - Inicializa agentes por request con `use_finetuned`
   - Agrega `model_used` en resultado

5. **backend/services/agents/extractor_agent.py**
   - Usa `self.complete()` en lugar de `self.llm.complete()`

6. **backend/services/agents/comparator_agent.py**
   - Usa `self.complete()` en lugar de `self.llm.complete()`

7. **backend/services/agents/risk_agent.py**
   - Usa `self.complete()` en lugar de `self.llm.complete()`

### Frontend

8. **frontend/contract-analysis.html**
   - Agregado checkbox "Use Fine-Tuned Model"
   - Descripci√≥n de diferencias entre modelos
   - Marcado por defecto (fine-tuned)

9. **frontend/js/contract-analysis.js**
   - Lee estado del checkbox `useFinetuned`
   - Env√≠a par√°metro `use_finetuned` en request

### Configuraci√≥n

10. **.env**
    - Agregado `LLM_PROVIDER=groq`
    - Agregado `GROQ_API_KEY=`
    - Agregado `OLLAMA_BASE_URL=http://localhost:11434`

11. **.env.example**
    - Documentaci√≥n de nuevas variables
    - Instrucciones para obtener Groq API key

12. **start-rag-system.bat**
    - Detecta provider (Groq o Ollama)
    - Valida GROQ_API_KEY si usa Groq
    - Valida Ollama si usa Ollama

### Documentaci√≥n

13. **README.md**
    - Actualizado Quick Start para Groq
    - Comparaci√≥n Groq vs Ollama
    - Nuevos troubleshooting

14. **CONFIGURACION_GROQ.md** (nuevo)
    - Gu√≠a completa de configuraci√≥n de Groq
    - C√≥mo obtener API key
    - Comparaci√≥n de caracter√≠sticas
    - L√≠mites y troubleshooting

15. **MODELO_FINE_TUNEADO.md**
    - Actualizado para reflejar uso de Groq
    - Explicaci√≥n de system prompts

16. **backend/dockerfile**
    - Agregado script de entrypoint para crear kaggle.json
    - Soluciona problema de autenticaci√≥n de Kaggle

## C√≥mo Funciona Ahora

### Flujo de An√°lisis

```
Usuario marca/desmarca checkbox
    ‚Üì
Frontend env√≠a use_finetuned: true/false
    ‚Üì
RAG Service recibe par√°metro
    ‚Üì
Orchestrator inicializa agentes con use_finetuned
    ‚Üì
Agentes llaman a LLM con use_finetuned
    ‚Üì
LLMService:
  - Si use_finetuned=true: Agrega system prompt especializado
  - Si use_finetuned=false: Usa prompt normal
    ‚Üì
Groq API procesa y devuelve resultado
```

### Diferencia entre Modelos

**Base Model (use_finetuned=false):**
```python
prompt = "Analiza este contrato: ..."
```

**Fine-Tuned (use_finetuned=true):**
```python
system_prompt = "You are a legal contract analysis expert..."
prompt = system_prompt + "\n\n" + "Analiza este contrato: ..."
```

## Configuraci√≥n Requerida

### Para usar Groq (Recomendado)

```env
LLM_PROVIDER=groq
GROQ_API_KEY=gsk_tu_api_key_aqui
```

Obtener key: https://console.groq.com/keys

### Para usar Ollama (Opcional)

```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
```

Requiere:
1. Instalar Ollama
2. `ollama pull llama3.2:3b`

## Ventajas de Groq

1. **Sin instalaci√≥n**: No descargas modelos (ahorra 5GB)
2. **M√°s r√°pido**: Inferencia ultra-r√°pida en GPUs de Groq
3. **Gratis**: Tier gratuito generoso
4. **Sin GPU local**: Funciona en cualquier PC
5. **Mismo modelo**: Llama 3.2 3B disponible

## Testing

### Test 1: Verificar Groq API

```bash
# En .env
LLM_PROVIDER=groq
GROQ_API_KEY=tu_key

# Iniciar RAG service
start-rag-system.bat

# Deber√≠a ver:
# ‚úì Groq API key configured
```

### Test 2: Analizar con Fine-Tuned

1. Abrir http://localhost:3000/contract-analysis.html
2. Marcar checkbox "Use Fine-Tuned Model"
3. Pegar contrato
4. Analizar
5. Verificar en logs: `model_used: 'fine-tuned'`

### Test 3: Analizar con Base Model

1. Desmarcar checkbox
2. Analizar mismo contrato
3. Verificar en logs: `model_used: 'base'`
4. Comparar resultados

### Test 4: Cambiar a Ollama

```bash
# En .env
LLM_PROVIDER=ollama

# Iniciar Ollama
ollama serve

# Descargar modelo
ollama pull llama3.2:3b

# Reiniciar RAG service
start-rag-system.bat

# Deber√≠a ver:
# ‚úì Ollama is running
```

## Pr√≥ximos Pasos

1. ‚úÖ Obtener Groq API key
2. ‚úÖ Configurar .env
3. ‚úÖ Iniciar sistema
4. ‚úÖ Probar ambos modelos
5. üìä Comparar resultados
6. üéØ Elegir el que mejor funcione

## Notas Importantes

- **Groq es el default**: M√°s f√°cil de configurar
- **Ollama es opcional**: Para privacidad total
- **Checkbox siempre visible**: Usuario decide qu√© modelo usar
- **Mismo c√≥digo**: Funciona con ambos providers
- **Sin fine-tuning real**: Usa system prompts (suficiente para la mayor√≠a de casos)

## Archivos que NO se Modificaron

- `models/lora_model/*` - Modelo LoRA sigue ah√≠ (para referencia)
- Backend Go - No requiere cambios
- Base de datos - Sin cambios
- Docker compose - Sin cambios

## Rollback (Si algo falla)

Para volver a Ollama local:

```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
```

Y seguir la gu√≠a original de `MODELO_FINE_TUNEADO.md`.

---

**Resumen:** Sistema actualizado para usar Groq API por defecto, con selector de modelo en UI, manteniendo compatibilidad con Ollama local.
